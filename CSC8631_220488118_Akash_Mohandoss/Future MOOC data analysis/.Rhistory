BCLF_COef[,1]
BCLF_COef[,75]
BCLF_COef[,100]
par(mfrow=c(1,1))
## Examine the effect of the tuning parameter on the parameter estimates
plot(BC_lasso_fit, xvar="lambda", col=1:9, label=TRUE)
lasso_cv_fit = cv.glmnet(x=train[,1:9], y=train[,10], family="binomial", alpha=1, standardize=FALSE, lambda=grid,type.measure="class")
lasso_cv_fit = cv.glmnet(x=matrix(train[,1:9]), y=train[,10], family="binomial", alpha=1, standardize=FALSE, lambda=grid,type.measure="class")
lasso_cv_fit = cv.glmnet(matrix(train[,1:9]), train[,10], family="binomial", alpha=1, standardize=FALSE, lambda=grid,type.measure="class")
lasso_cv_fit = cv.glmnet(as.matrix(train[,1:9]), train[,10], family="binomial", alpha=1, standardize=FALSE, lambda=grid,type.measure="class")
plot(lasso_cv_fit)
#Cross-validation
lasso_cv_fit = cv.glmnet(as.matrix(train[,1:9]), train[,10], alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_cv_fit)
#Cross-validation
lasso_cv_fit = cv.glmnet(as.matrix(train[,1:9]), family="binomial",train[,10], alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_cv_fit)
#Cross-validation
lasso_cv_fit = cv.glmnet(as.matrix(train[,1:9]), family="binomial",train[,10], alpha=1, standardize=FALSE, lambda=grid,type.measure="class")
plot(lasso_cv_fit)
lasso_cv_fit = cv.glmnet(as.matrix(train[,1:9]), family="binomial",train[,10], alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_cv_fit)
(lambda_lasso_min = lasso_cv_fit$lambda.min)
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
# Find the parameter estimates associated with optimal value of the tuning parameter
coef(BCData_lasso_fit, s=lambda_lasso_min)
# Find the parameter estimates associated with optimal value of the tuning parameter
coef(BC_lasso_fit, s=lambda_lasso_min)
which_lambda_lasso
lambda_lasso_min
(which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min))
library(glmnet)
## Choose grid of values for the tuning parameter
grid = 10^seq(5, -3, length.out=100)
## Fit a model with LASSO penalty for each value of the tuning parameter
BC_lasso_fit = glmnet(x=FinalBCDS[,1:9], y=FinalBCDS[,10], family="binomial", alpha=1, standardize=FALSE, lambda=grid)
BCLF_COef=coef(BC_lasso_fit)
dim(BCLF_COef)
BCLF_COef[,1]
BCLF_COef[,75]
BCLF_COef[,100]
par(mfrow=c(1,1))
## Examine the effect of the tuning parameter on the parameter estimates
plot(BC_lasso_fit, xvar="lambda", col=1:9, label=TRUE)
#Cross-validation in order choose tuning parameter (lambda)
lasso_cv_fit = cv.glmnet(as.matrix(FinalBCDS[,1:9]), family="binomial",FinalBCDS[,10], alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_cv_fit)
# Identify the optimal value for the tuning parameter
(lambda_lasso_min = lasso_cv_fit$lambda.min)
(which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min))
# Find the parameter estimates associated with optimal value of the tuning parameter
coef(BC_lasso_fit, s=lambda_lasso_min)
lasso_cv_train = cv.glmnet(train[.1:9], train[,10],
family="binomial", alpha=1, standardize=FALSE, lambda=grid,)
## Perform cross-validation over the training data to select tuning parameter
lasso_cv_train = cv.glmnet(as.matrix(train[,1:9]), train[,10],
family="binomial", alpha=1, standardize=FALSE, lambda=grid,)
(lambda_lasso_min_train = lasso_cv_train$lambda.min)
which_lambda_lasso_train = which(lasso_cv_train$lambda == lambda_lasso_min_train)
which_lambda_lasso_train
which_lambda_lasso
plot(lasso_cv_train)
plot(lasso_cv_fit)
plot(lasso_cv_train)
which_lambda_lasso_train
lasso_train = glmnet(train[,1:9], train[,10], family="binomial",
alpha=1, standardize=FALSE, lambda=lambda_lasso_min_train)
## Compute fitted values for the validation data:
phat_test = predict(lasso_train, train[,1:9], s=lambda_lasso_min_train,
type="response")
lasso_train = glmnet(as.matrix(train[,1:9]), train[,10], family="binomial",
alpha=1, standardize=FALSE, lambda=lambda_lasso_min_train)
## Compute fitted values for the validation data:
phat_test = predict(lasso_train, train[,1:9], s=lambda_lasso_min_train,
type="response")
phat_test = predict(as.matrix(lasso_train), train[,1:9], s=lambda_lasso_min_train,
type="response")
# Compute fitted values for the validation data:
phat_test = predict(lasso_train, as.matrix(train[,1:9]), s=lambda_lasso_min_train,
type="response")
yhat_test = ifelse(phat_test > 0.5, 1, 0)
## Compute test error
1 - mean(BC_y == yhat_test)
## Compute test error
1 - mean(train[,10] == yhat_test)
library('caret')
set.seed(99)
#Splitting our data into 70% training and 30% testing data sets
library(caret)
training <- createDataPartition(FinalBCDS$Class, p=0.7, list=FALSE)
train <- FinalBCDS[ training, ]
test <- FinalBCDS[ -training, ]
table(train$Class)
table(test$Class)
#Logit Model fit using glm function for the train samples
breast_cancer_glm <- glm(Class ~ ., data = train, family = "binomial")
summary(breast_cancer_glm)
test_prob <- predict(breast_cancer_glm, test, type="response")
test_pred <- ifelse(test_prob < 0.5, 0, 1)
test_acc <- mean(test_pred == test$Class)
#plot the logistic fit model for the unregularised data
par(mfrow=c(1,2))
plot(breast_cancer_glm, which=1:2)
#Accuracy of unregularized Data
cat("Testing Accuracy: ", test_acc)
bss_fit_AIC = bestglm(FinalBCDS, family=binomial, IC="AIC")
library("mlbench")
list.of.packages <- c("dplyr", "qwraps2", "ggplot2", "gridExtra", "car","arm","psych","caTools")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
options(qwraps2_markup = "markdown")
library("keras")
library("corrplot")
library("pROC")
library('ggplot2')
library("MASS")
library("caTools")
library("lattice")
library("corrgram")
library("ROCR")
library("ggcorrplot")
library('caret')
library("PerformanceAnalytics")
library("bestglm")
install.packages('pscl')
library('pscl')
library('nclSLR')
library('car')
library('glmnet')
--------------------
jhb
# Apply best subset selection
bss_fit_AIC = bestglm(FinalBCDS, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(FinalBCDS, family=binomial, IC="BIC")
#Subsets displayed from the results
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
# Create multi-panel plotting device
par(mfrow=c(1,2))
# Producing plots highlighting optimal value of k
plot(0:bc_p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:bc_p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
# Check which predictors are in the 6-predictor model
pstar = 6
bss_fit_AIC$Subsets[pstar+1,]
# Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(bc_p+1)]))
BC_data_red = data.frame(BC_X1[,indices], BC_y)
training1 <- createDataPartition(BC_data_red$BC_y, p=0.7, list=FALSE)
train1 <- BC_data_red[ training1, ]
test1 <- BC_data_red[ -training1, ]
table(train1$BC_y)
table(test1$BC_y)
logreg1_fit = glm(BC_y ~ ., data=train, family="binomial")
BC_data_red = data.frame(BC_X1[,indices], BC_y)
training1 <- createDataPartition(BC_data_red$BC_y, p=0.7, list=FALSE)
train1 <- BC_data_red[ training1, ]
test1 <- BC_data_red[ -training1, ]
table(train1$BC_y)
table(test1$BC_y)
logreg1_fit = glm(BC_y ~ ., data=train1, family="binomial")
summary(logreg1_fit)
test_prob1 <- predict(logreg1_fit, test1, type="response")
test_pred1 <- ifelse(test_prob1 < 0.5, 0, 1)
test_acc1 <- mean(test_pred1 == test1$BC_y)
cat("Test Accuracy: ", test_acc1)
pR2(logreg1_fit)['McFadden']
BC_data_red = data.frame(BC_X1[,indices], BC_y)
training1 <- createDataPartition(BC_data_red$BC_y, p=0.7, list=FALSE)
train1 <- BC_data_red[ training1, ]
test1 <- BC_data_red[ -training1, ]
table(train1$BC_y)
table(test1$BC_y)
logreg1_fit = glm(BC_y ~ ., data=train1, family="binomial")
summary(logreg1_fit)
test_prob1 <- predict(logreg1_fit, test1, type="response")
test_pred1 <- ifelse(test_prob1 < 0.5, 0, 1)
test_acc1 <- mean(test_pred1 == test1$BC_y)
cat("Test Accuracy: ", test_acc1)
pR2(logreg1_fit)['McFadden']
set.seed(99)
#Splitting our data into 70% training and 30% testing data sets
library(caret)
training <- createDataPartition(FinalBCDS$Class, p=0.7, list=FALSE)
train <- FinalBCDS[ training, ]
test <- FinalBCDS[ -training, ]
table(train$Class)
table(test$Class)
#Logit Model fit using glm function for the train samples
breast_cancer_glm <- glm(Class ~ ., data = train, family = "binomial")
summary(breast_cancer_glm)
test_prob <- predict(breast_cancer_glm, test, type="response")
test_pred <- ifelse(test_prob < 0.5, 0, 1)
test_acc <- mean(test_pred == test$Class)
#plot the logistic fit model for the unregularised data
par(mfrow=c(1,2))
plot(breast_cancer_glm, which=1:2)
#Accuracy of unregularized Data
cat("Testing Accuracy: ", test_acc)
#McFadden R Square-test for evaluating predictive power
pR2(breast_cancer_glm)['McFadden']
#calculate VIF values for each predictor variable in our model
vif(breast_cancer_glm)
training1 <- createDataPartition(BC_data_red$BC_y, p=0.7, list=FALSE)
train1 <- BC_data_red[ training1, ]
test1 <- BC_data_red[ -training1, ]
table(train1$BC_y)
table(test1$BC_y)
logreg1_fit = glm(BC_y ~ ., data=train1, family="binomial")
summary(logreg1_fit)
test_prob1 <- predict(logreg1_fit, test1, type="response")
test_pred1 <- ifelse(test_prob1 < 0.5, 0, 1)
test_acc1 <- mean(test_pred1 == test1$BC_y)
cat("Test Accuracy: ", test_acc1)
pR2(logreg1_fit)['McFadden']
set.seed(99)
# Apply best subset selection
bss_fit_AIC = bestglm(FinalBCDS, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(FinalBCDS, family=binomial, IC="BIC")
#Subsets displayed from the results
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
# Create multi-panel plotting device
par(mfrow=c(1,2))
# Producing plots highlighting optimal value of k
plot(0:bc_p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:bc_p, bss_fit_BIC$Subsets$BIC, xlab="Number of predictors", ylab="BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
# Check which predictors are in the 6-predictor model
pstar = 6
bss_fit_AIC$Subsets[pstar+1,]
# Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(bc_p+1)]))
BC_data_red = data.frame(BC_X1[,indices], BC_y)
training1 <- createDataPartition(BC_data_red$BC_y, p=0.7, list=FALSE)
train1 <- BC_data_red[ training1, ]
test1 <- BC_data_red[ -training1, ]
table(train1$BC_y)
table(test1$BC_y)
logreg1_fit = glm(BC_y ~ ., data=train1, family="binomial")
summary(logreg1_fit)
test_prob1 <- predict(logreg1_fit, test1, type="response")
test_pred1 <- ifelse(test_prob1 < 0.5, 0, 1)
test_acc1 <- mean(test_pred1 == test1$BC_y)
cat("Test Accuracy: ", test_acc1)
pR2(logreg1_fit)['McFadden']
set.seed(99)
lda1 = lda(Class ~ . , data = train,family=binomial)
lda_predict = predict(lda1,test)
lda_predict
head(lda_predict$class)
library(nclSLR)
## Perform LDA on the training data - note that we need to convert the vector of predictors
## into a matrix because the linDA function expects its variables argument to be a matrix
## or data frame
linDA(variables=matrix(BC_data_red, ncol=1),
group=BC_data_red$BC_y)
library(MASS)
## Fit the LDA classifier using the training data
lda_train = lda(BC_y~., data=train1)
## Compute fitted values for the validation data
lda_test = predict(lda_train, test1)
yhat_test = lda_test$class
## Calculate (test) confusion matrix
(confusion = table(Observed=train1, Predicted=yhat_test))
BC_predictions
## Calculate (test) confusion matrix
(confusion = table(Observed=as.facotr(train1), Predicted=as.factor(yhat_test))
## Calculate (test) confusion matrix
(confusion = table(Observed=as.factor(train1), Predicted=as.factor(yhat_test)))
jhbn
## Calculate (test) confusion matrix
(confusion = table(Observed=as.factor(train1), Predicted=as.factor(yhat_test)))
(confusion = table(Observed=BC_y, Predicted=yhat_test))
## Calculate (test) confusion matrix
(confusion = table(Observed=train1$BC_y, Predicted=yhat_test))
## Perform LDA on the training data - note that we need to convert the vector of predictors
## into a matrix because the linDA function expects its variables argument to be a matrix
## or data frame
linDA(variables=matrix(BC_data_red, ncol=1),group=BC_data_red$BC_y)
set.seed(99)
lda1 = lda(Class ~ . , data = train,family=binomial)
lda1
#Linear Discriminant Analysis
set.seed(99)
lda1_fit = lda(Class ~ . , data = train,family=binomial)
lda1_fit
lda_predict = predict(lda1_fit,test)
lda_predict
head(lda_predict$class)
mean(lda_predict$class==test$Class)
yhat = ifelse(lda_predict$x>0.5,1,0)
err = 1 - mean(test$Class == yhat)
table(lda.class,test$Class)
lda.class = lda_predict$class
mean(lda_predict$class==test$Class)
yhat = ifelse(lda_predict$x>0.5,1,0)
err = 1 - mean(test1$Class == yhat)
table(lda.class,test1$Class)
lda.class = lda_predict$class
mean(lda_predict$class==test1$Class)
yhat = ifelse(lda_predict$x>0.5,1,0)
err = 1 - mean(test1$Class == yhat)
table(lda.class,test1$Class)
mean(lda_predict$class==test1$Class)
set.seed(99)
lda1_fit = lda(Class ~ . , data = train,family=binomial)
lda1_fit
lda_predict = predict(lda1_fit,test)
#displaying predicted class for first six observations in test data
head(lda_predict$class)
names(lda_predict)
lda.class = lda_predict$class
mean(lda_predict$class==test1$Class)
mean(lda.class==test1$Class)
set.seed(99)
lda1_fit = lda(BC_y ~ . , data = train1,family=binomial)
lda1_fit
lda_predict = predict(lda1_fit,test1)
#displaying predicted class for first six observations in test data
head(lda_predict$class)
names(lda_predict)
lda.class = lda_predict$class
mean(lda.class==test1$Class)
yhat = ifelse(lda_predict$x>0.5,1,0)
err = 1 - mean(test1$Class == yhat)
table(lda.class,test1$Class)
confusionMatrix(lda.class,test1$Class)
confusionMatrix(as.factor(lda.class),as.factor(test1$Class))
confusionMatrix(as.factor(ywhat),as.factor(test1$Class))
confusionMatrix(as.factor(yhat),as.factor(test1$Class))
mean(lda.class==test1$Class)
set.seed(99)
lda1_fit = lda(Class ~ . , data = train,family=binomial)
lda1_fit
lda_predict = predict(lda1_fit,train)
lda_predict
lda_predict = predict(lda1_fit,train)
table(Predicted=lda1_fit$class,Class=Class)
lda_predict = predict(lda1_fit,data=train)
set.seed(99)
lda1_fit = lda(Class ~ . , data = train,family=binomial)
lda_predict = predict(lda1_fit,data=train)
lda1_fit = lda(Class ~ . , data = train)
lda1_fit
lda_predict = predict(lda1_fit,data=train)
table(Predicted=lda1_fit$class,Class=Class)
table(Predicted=lda_predict$class,Class=Class)
table(Predicted=lda_predict$class,Class=train$Class)
confusionMatrix(Predicted=lda_predict$class,Class=train$Class)
confusionMatrix(Predicted=as.factor(lda_predict$class),Class=as.factor(train$Class))
table(Predicted=lda_predict$class,Class=train$Class)
table(Predicted=lda_predict$class,Observed=train$Class)
lda_predict_train = predict(lda1_fit,data=train)
table(Predicted=lda_predict$class,Observed=train$Class)
head(lda_predict$class)
lda_predict_train = predict(lda1_fit,data=train)
table(Predicted=lda_predict_train$class,Observed=train$Class)
lda_predict_test = predict(lda1_fit,data=test)
table(Predicted=lda_predict_test$class,Observed=test$Class)
table(Predicted=as.factor(lda_predict_test$class),Observed=as.factor(test$Class))
confusionMatrix(Predicted=as.factor(lda_predict_test$class),Observed=as.factor(test$Class))
table(Predicted=lda_predict_test$class,Observed=test$Class)
plotROC(test$Class,BC_pred_rr)
library("ROCR")
plotROC(test$Class,BC_pred_rr)
library("pROC")
plotROC(test$Class,BC_pred_rr)
par(mfrow = c(1,1))
plot(BC_perf_rr,colorize=T, main = "Regularized Logit Performance")
set.seed(99)
# Apply best subset selection
bss_fit_AIC = bestglm(FinalBCDS, family=binomial, IC="AIC")
bss_fit_BIC = bestglm(FinalBCDS, family=binomial, IC="BIC")
#Subsets displayed from the results
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## Identify best-fitting models
(best_AIC = bss_fit_AIC$ModelReport$Bestk)
(best_BIC = bss_fit_BIC$ModelReport$Bestk)
# Create multi-panel plotting device
par(mfrow=c(1,2))
# Producing plots highlighting optimal value of k
plot(0:bc_p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
pstar = 6
bss_fit_AIC$Subsets[pstar+1,]
# Construct a reduced data set containing only the selected predictor
(indices = as.logical(bss_fit_AIC$Subsets[pstar+1, 2:(bc_p+1)]))
BC_data_red = data.frame(BC_X1[,indices], BC_y)
training1 <- createDataPartition(BC_data_red$BC_y, p=0.7, list=FALSE)
train1 <- BC_data_red[ training1, ]
test1 <- BC_data_red[ -training1, ]
table(train1$BC_y)
table(test1$BC_y)
logreg1_fit = glm(BC_y ~ ., data=train1, family="binomial")
summary(logreg1_fit)
test_prob1 <- predict(logreg1_fit, test1, type="response")
test_pred1 <- ifelse(test_prob1 < 0.5, 0, 1)
test_acc1 <- mean(test_pred1 == test1$BC_y)
cat("Test Accuracy: ", test_acc1)
pR2(logreg1_fit)['McFadden']
set.seed(99)
# Ridge Regression
grid=10^seq(5,-3,length=100)
#fit ridge regression model
par(mfrow=(c(1,1)))
BC_ridge <- glmnet(train[,1:9], train[,10], alpha = 0,family = "binomial",lambda = grid)
plot(BC_ridge,xvar = "lambda",col=1:9,label = T)
#perform k-fold cross-validation to find optimal lambda value
BC_cv_ridge <- cv.glmnet(as.matrix(train[,1:9]), train[,10], alpha = 0,family = "binomial",lambda = grid)
plot(BC_cv_ridge)
#find optimal lambda value that minimizes test MSE
BCbest_lambda_ridge <- BC_cv_ridge$lambda.min
BCbest_lambda_ridge
coef(BC_ridge,s=BCbest_lambda_ridge)
# Which tuning parameter was the minimum?
(i = which(BC_cv_ridge$lambda == BC_cv_ridge$lambda.min))
# Extract corresponding mean MSE
BC_cv_ridge$cvm[i]
#The lambda value that minimizes the test MSE turns out to be 0.005336699
#find coefficients of best fit model
best_model_ridge <- glmnet(train[,1:9], train[,10], alpha = 0, lambda = BCbest_lambda_ridge, family = "binomial")
coef(best_model_ridge)
#find prediction and accuracy
BC_prediction_tr <- predict(best_model_ridge,newx = as.matrix(test[,1:9]),s = BCbest_lambda_ridge)
BC_predictions <- rep(0, nrow(test))
BC_predictions[BC_prediction_tr >0.5] = 1
#tabulating the confusion matrix
conf_mat <- confusionMatrix(as.factor(BC_predictions), as.factor(test$Class))
conf_mat
#Prediction for the test data for the model
BC_pred_rr <- prediction(BC_prediction_tr, test$Class)
BC_perf_rr <- performance(BC_pred_rr, measure = "tpr", x.measure = "fpr")
#finding the auc value to estimate the accuracy
BC_auc_rr <- performance(BC_pred_rr, measure = "auc")
BC_auc_rr <- BC_auc_rr@y.values[[1]]
BC_auc_rr
#Plotting the AUC curve
par(mfrow = c(1,1))
plot(BC_perf_rr,colorize=T, main = "Regularized Logit Performance")
auc <- round(BC_auc_rr, 4)
legend(.5, .4, BC_auc_rr, title = "AUC", cex = 1)
lda_predict_train = predict(lda1_fit,data=test)
table(Predicted=lda_predict_train$class,Observed=test$Class)
lda1_fit = lda(Class ~ . , data = train)
lda1_fit
lda_predict = predict(lda1_fit,test)
names(lda_predict)
lda.class = lda_predict$class
yhat = ifelse(lda_predict$x>0.5,1,0)
err = 1 - mean(test_lda$Class == yhat)
err = 1 - mean(test$Class == yhat)
err
yhat
table(lda.class,test$Class)
mean(lda.class==test$Class)
confusionMatrix(lda.class,test$Class)
confusionMatrix(as.factor(lda.class),as.factor(test$Class))
set.seed(99)
#Fit LDA model
lda1_fit = lda(Class ~ . , data = train)
#model output
lda1_fit
#prediction over the test data for the model
lda_predict = predict(lda1_fit,test)
#displays the names of the columns available
names(lda_predict)
lda.class = lda_predict$class
yhat = ifelse(lda_predict$x>0.5,1,0)
#find the err value
err = 1 - mean(test$Class == yhat)
#displaying the confusion matrix for the model over the test data
confusionMatrix(as.factor(lda.class),as.factor(test$Class))
#Calculate the accuracy of the model
mean(lda.class==test$Class)
#displaying the confusion matrix for the model over the test data
confusionMatrix(Observed=as.factor(lda.class),Predicted=as.factor(test$Class))
confusionMatrix(as.factor(lda.class),as.factor(test$Class))
ggcorrplot(cor(FinalBCDS[-1]), type = 'lower', lab = TRUE) +
ggtitle("Correlation Plot of All Covariates") +
theme(plot.title = element_text(hjust = 0.5, size = 22))
ggcorrplot(cor(FinalBCDS[-1]), type = 'lower', lab = TRUE) +
ggtitle("Correlation Plot of All Covariates") +
theme(plot.title = element_text(hjust = 0.5, size = 22))
library('ggplot2')
ggcorrplot(cor(FinalBCDS[-1]), type = 'lower', lab = TRUE) +
ggtitle("Correlation Plot of All Covariates") +
theme(plot.title = element_text(hjust = 0.5, size = 22))
install.packages(ggplot2)
install.packages('ggplot2')
corrplot(corMatMy, order = "hclust")
install.packages("ggplot2")
corrplot(corMatMy, order = "hclust")
--------------------------------------------------------------------------------
#Loading the libraries
library("mlbench")
library("mlbench")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
Combined_table1 <-rbind(cyber.security.1_enrolments,cyber.security.2_enrolments,cyber.security.3_enrolments,
cyber.security.4_enrolments,cyber.security.5_enrolments,cyber.security.6_enrolments,cyber.security.7_enrolments)
library(ProjectTemplate)
load.project()
